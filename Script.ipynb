{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hide the API key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.get(\"youtube_api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"AIzaSyAlAjDhaQ2iZ8u6Z-Zzmv3FezFPiOIG2rs\"\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey = api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the wrapper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the ID of the playlist containing all videos a channel has uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uploads_playlist_id(username):\n",
    "    \n",
    "    request = youtube.channels().list(\n",
    "        part = \"contentDetails\",\n",
    "        forUsername = username)\n",
    "    \n",
    "    response = request.execute()\n",
    "    \n",
    "    return response[\"items\"][0][\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the IDs of the uploaded videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YouTube won't let users retrieve data for more than 50 videos within a single API call. Luckily though the response from the API provides the 'nextPageToken' key which we will allow us to access the page after the one we just got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_video_ids(uploaded_videos_playlist_id): # The playlist ID we downloaded with the previous function\n",
    "    \n",
    "    video_ids = []\n",
    "    \n",
    "    page_token = None # For the first iteration of the loop the page_token is set to 'None' as we're getting the first 50 ids\n",
    "    \n",
    "    while True: \n",
    "        \n",
    "        request = youtube.playlistItems().list(\n",
    "        part = \"id, contentDetails\",\n",
    "        playlistId = uploaded_videos_playlist_id,\n",
    "        maxResults = 50,\n",
    "        pageToken = page_token) # Setting the 'pageToken' argument equal to the page_token we just saved from the previous iteration\n",
    "        \n",
    "        response = request.execute()\n",
    "        \n",
    "        for item in response[\"items\"]:\n",
    "            video_ids.append(item[\"contentDetails\"][\"videoId\"]) # Appending the ids \n",
    "            \n",
    "        if 'nextPageToken' in response.keys():       # Is there a 'nextPageToken' key in the json object? \n",
    "            page_token = response['nextPageToken']   # If so, let's save the page token so that we can move to the next iteration.\n",
    "        else:                                        # Else, let's break out of the loop and return our output\n",
    "            return video_ids                               \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data for each uploaded video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_video_data_in_df(username):\n",
    "    \n",
    "    vid_ids = get_all_video_ids(get_uploads_playlist_id(username))\n",
    "    \n",
    "    # Let's split the vid_ids list into chunks. The number of chunks is equal to the total n of vids uploaded \n",
    "    # by a channel divided by 50 (the max amount of data retrievable in one call), rounded to the greatest near integer \n",
    "    # and then cast to an integer\n",
    "    \n",
    "    n_chunks = int(np.ceil(len(vid_ids)/50))\n",
    "    \n",
    "    splits = np.array_split(vid_ids, n_chunks) \n",
    "    \n",
    "    # Let's create an empty dataframe which it'll store all our data\n",
    "    \n",
    "    df = pd.DataFrame(columns = [\"id\", \"title\", \"published_at\", \"view_count\", \"like_count\", \"dislike_count\", \n",
    "                             \"comment_count\", \"duration\", \"tags\", \"last_updated\"])\n",
    "    \n",
    "    for split in splits:  \n",
    "        request = youtube.videos().list(\n",
    "        part = \"snippet, statistics, contentDetails\",\n",
    "        id = ','.join(split)) # The lists of video IDS need to be collapsed into a single string with values separated by a comma\n",
    "        \n",
    "        response = request.execute()\n",
    "             \n",
    "        for video in response[\"items\"]:\n",
    "            df = df.append({\"id\": video[\"id\"],\n",
    "                   \"title\": video[\"snippet\"][\"title\"], \n",
    "                   \"published_at\": video[\"snippet\"][\"publishedAt\"],\n",
    "                   \"tags\": \";\".join(video[\"snippet\"][\"tags\"]) if \"tags\" in video[\"snippet\"].keys() else np.nan,\n",
    "                   \"view_count\": video[\"statistics\"][\"viewCount\"],\n",
    "                   \"like_count\": video[\"statistics\"][\"likeCount\"],\n",
    "                   \"dislike_count\": video[\"statistics\"][\"dislikeCount\"],\n",
    "                   \"comment_count\": video[\"statistics\"][\"commentCount\"] if \"commentCount\" in video[\"statistics\"].keys() else np.nan,\n",
    "                   \"duration\": video[\"contentDetails\"][\"duration\"],\n",
    "                   \"last_updated\": date.today()\n",
    "                   },\n",
    "                   ignore_index = True)\n",
    "            \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = store_video_data_in_df(\"joshstarmer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicated ids: great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"id\"].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our variables are of the object type. We need to cast the non-string ones to the correty datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the columns with the word \"count\" in their name need to be cast as numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [col for col in df.columns if \"count\" in col]\n",
    "\n",
    "df[numeric_cols] = df[numeric_cols].apply(lambda x: pd.to_numeric(arg = x, downcast = \"integer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the \"published_at\" column on the letter \"T\" and keep the first resulting column (the date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"published_at\"] = df[\"published_at\"].str.split(\"T\", expand = True).iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cast these two variables as datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_cols = [\"published_at\", \"last_updated\"]\n",
    "\n",
    "df[datetime_cols] = df[datetime_cols].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix the \"duration\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"duration\"] = df[\"duration\"].str.replace(\"PT\", \"\").str.replace(\"H\",\" hours \").str.replace(\"M\", \" minutes \").str.replace(\"S\", \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"duration\"] = pd.to_timedelta(df[\"duration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with another YouTuber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.DataFrame(columns = [\"id\", \"title\", \"published_at\", \"view_count\", \"like_count\", \"dislike_count\", \n",
    "                             \"comment_count\", \"duration\", \"tags\", \"last_updated\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store_video_data_in_df(\"RedLetterMedia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
